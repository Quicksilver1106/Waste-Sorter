# -*- coding: utf-8 -*-
"""Untitled2.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Z0gcsFBVV_F9cTEj6YzscLXQZGWiZ6yq
"""



import torch
import torchvision.transforms as transforms
from PIL import Image

# Define the transform to be applied to the input image
transform = transforms.Compose([
    transforms.Resize(224),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# Load the trained model
the_model = torch.load('my_model.pth')

# Set the model to evaluation mode
model.eval()

# Load and transform the input image
img = Image.open('glass29.jpg')
img = transform(img)

# Check if the state dictionary keys match with the model definition
model_dict = model.state_dict()
for k, v in state_dict.items():
    if k not in model_dict:
        print(f"Unexpected key: {k}")
    elif v.shape != model_dict[k].shape:
        print(f"Mismatched shape for key {k}: model has shape {model_dict[k].shape}, while state_dict has shape {v.shape}")
    else:
        model_dict[k] = v


# Add a batch dimension to the input image and pass it through the model
output = model(img.unsqueeze(0))

# Get the predicted class
predicted_class = torch.argmax(output)

# Print the predicted class
print(predicted_class.item())